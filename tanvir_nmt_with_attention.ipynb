{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qNSzzyaCbD"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAmSR1FaqKrl"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGFTkuRvzWqc"
      },
      "outputs": [],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPJ9J7iPUchc"
      },
      "outputs": [],
      "source": [
        "use_builtins = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqFqKi4fqN9X"
      },
      "outputs": [],
      "source": [
        "#@title Shape checker\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  train_data = '/content/sentencesTrain.txt'\n",
        "  test_data = '/content/tokensTrain.txt'\n",
        "\n",
        "  targ = []\n",
        "  inp = []\n",
        "\n",
        "  lines = []\n",
        "  with open(train_data, encoding='cp1252') as f:\n",
        "    lines = f.readlines()\n",
        "  \n",
        "  for sentence in lines:\n",
        "    inp.append(sentence.replace('\\n', ''))\n",
        "\n",
        "  with open(test_data, encoding='cp1252') as f:\n",
        "    lines = f.readlines()\n",
        "  \n",
        "  for sentence in lines:\n",
        "    targ.append(sentence.replace(',', ' ').replace('\\n', ''))\n",
        "\n",
        "  return targ, inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTbSbBz55QtF"
      },
      "outputs": [],
      "source": [
        "targ, inp = load_data()\n",
        "print(inp[-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp)"
      ],
      "metadata": {
        "id": "d_b4eMUmZbkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(targ)"
      ],
      "metadata": {
        "id": "5Fd-8yn_ZP-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH_dPY8TRp3c"
      },
      "outputs": [],
      "source": [
        "print(targ[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(inp)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc6-NK1GtWQt"
      },
      "outputs": [],
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD0e-DWGQ2Vo"
      },
      "outputs": [],
      "source": [
        "example_text = tf.constant('¿Todavía está en casa?')\n",
        "\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chTF5N885F0P"
      },
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UREvDg3sEKYa"
      },
      "outputs": [],
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmsI1Yql8FYe"
      },
      "outputs": [],
      "source": [
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlC4xuZnKLBS"
      },
      "outputs": [],
      "source": [
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KZxj8IrNZ9S"
      },
      "outputs": [],
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98g9rcxGQY0I"
      },
      "outputs": [],
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jx4Or_eFRSz"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9uNz3-IrF-"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60gSVh05Jl6l"
      },
      "outputs": [],
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "momiE59lXo6U"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4QMlOp8Gidh"
      },
      "outputs": [],
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYSHqmORgVFo"
      },
      "outputs": [],
      "source": [
        "(example_tokens != 0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y7hjPkNMmHh"
      },
      "outputs": [],
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqr8XGsAJlf6"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuzrCdmYlTcJ"
      },
      "outputs": [],
      "source": [
        "attention_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIMwC-f-ZC8N"
      },
      "outputs": [],
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysWDPO6hOS8X"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZsQJMqNmg_L"
      },
      "source": [
        "Here is the `Decoder` class and its initializer. The initializer creates all the necessary layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erYvHIgAl8kh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WfSIb2sArRT"
      },
      "outputs": [],
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJOi5btHAPNK"
      },
      "outputs": [],
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay_mTMPfnb2a"
      },
      "outputs": [],
      "source": [
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arTOBklcFTiC"
      },
      "source": [
        "The **encoder** processes its full input sequence with a single call to its RNN. This implementation of the **decoder** _can_ do that as well for efficient training. But this tutorial will run the decoder in a loop for a few reasons:\n",
        "\n",
        "* Flexibility: Writing the loop gives you direct control over the training procedure.\n",
        "* Clarity: It's possible to do masking tricks and use `layers.RNN`, or `tfa.seq2seq` APIs to pack this all into a single call. But writing it out as a loop may be clearer. \n",
        "  * Loop free training is demonstrated in the [Text generation](text_generation.ipynb) tutiorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-mLAcUEXpK"
      },
      "source": [
        "Now try using this decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZUMbYXIEVeA"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPnaw583CpnY"
      },
      "source": [
        "The decoder takes 4 inputs.\n",
        "\n",
        "* `new_tokens` -  The last token generated. Initialize the decoder with the `\"[START]\"` token.\n",
        "* `enc_output` - Generated by the `Encoder`.\n",
        "* `mask` - A boolean tensor indicating where `tokens != 0`\n",
        "* `state` - The previous `state` output from the decoder (the internal state\n",
        "  of the decoder's RNN). Pass `None` to zero-initialize it. The original\n",
        "  paper initializes it from the encoder's final RNN state. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u6eJBU4GL40"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5hqvbR5FUCD"
      },
      "outputs": [],
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "outputs": [],
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKXTLYu4IV7I"
      },
      "outputs": [],
      "source": [
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX1VF9XDJTOM"
      },
      "outputs": [],
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1rs0XL7Y2aS"
      },
      "outputs": [],
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWIyuy71TkJT"
      },
      "outputs": [],
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlYE68wzXoA8"
      },
      "outputs": [],
      "source": [
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHy6hzStrgjQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs_gsISsYPpY"
      },
      "outputs": [],
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGwWHIxLrjGR"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._train_step = _train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VrzgwztXzYJ"
      },
      "outputs": [],
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj3I7VULrk1R"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WACCHvKWBQ9C"
      },
      "source": [
        "### Test the training step\n",
        "\n",
        "Build a `TrainTranslator`, and configure it for training using the `Model.compile` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA6bCske8TXm"
      },
      "outputs": [],
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y5OnZDsB3sB"
      },
      "source": [
        "Test out the `train_step`. For a text model like this the loss should start near:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHe-OudqCFGK"
      },
      "outputs": [],
      "source": [
        "np.log(output_text_processor.vocabulary_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwMU9cFEfjha"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-xqtsMbCUp2"
      },
      "source": [
        "While it's easier to debug without a `tf.function` it does give a performance boost. So now that the `_train_step` method is working, try the `tf.function`-wrapped `_tf_train_step`, to maximize performance while training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFUsTKQx0jaH"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-bgU59jrztQ"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC8bRv_Gr3H9"
      },
      "outputs": [],
      "source": [
        "translator.use_tf_function = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKMYNF_sIFb9"
      },
      "source": [
        "The first call will be slow, because it traces the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLQZsX2dp1QK"
      },
      "outputs": [],
      "source": [
        "translator.train_step([example_input_batch, example_target_batch])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3t2Hg7UISYi"
      },
      "source": [
        "But after that it's usually 2-3x faster than the eager `train_step` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzXXMwjXCqqh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIvigTqaEcu1"
      },
      "source": [
        "A good test of a new model is to see that it can overfit a single batch of input. Try it, the loss should quickly go to zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-dIWMIBqK7b"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI02XFjoEt1k"
      },
      "source": [
        "Now that you're confident that the training step is working, build a fresh copy of the model to train from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emgfgh4tAmJt"
      },
      "outputs": [],
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7m4mtnj80sq"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQd_esVVoSf3"
      },
      "outputs": [],
      "source": [
        "train_translator.fit(dataset, epochs=50,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38rLdlmtQHCm"
      },
      "outputs": [],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO-CLL1LVBbM"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBQzFZ9uWU79"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IjwKTwtmdFf"
      },
      "outputs": [],
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "912aV0K7r90w"
      },
      "outputs": [],
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWCMHdoS32QN"
      },
      "outputs": [],
      "source": [
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lfuj3GcdD6e"
      },
      "outputs": [],
      "source": [
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "  \n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DpDnBdBdL9_"
      },
      "outputs": [],
      "source": [
        "Translator.sample = sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwLT0nxXym80"
      },
      "outputs": [],
      "source": [
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmOvVrZmwAxg"
      },
      "outputs": [],
      "source": [
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOmd8Y269MG3"
      },
      "outputs": [],
      "source": [
        "Translator.translate = translate_unrolled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd2rgyHwVVrv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
        "    'Esta es mi vida.', # \"This is my life.\"\"\n",
        "])\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = 'sentencesTest.txt'\n",
        "filepath = 'tanvir_ver_1.txt'\n",
        "\n",
        "out = open(filepath, 'a')\n",
        "\n",
        "with open(test_path, encoding = 'unicode_escape') as f:\n",
        "    lines = f.read().split('\\n')[:-1]\n",
        "    for line in lines:\n",
        "        list_data = []\n",
        "        list_data.append(line)\n",
        "        input_text = tf.constant(list_data)\n",
        "        result = translator.translate(input_text = input_text)\n",
        "        translated = result['text'][0].numpy().decode().replace(' ', ',').upper()\n",
        "        out.write(translated + '\\n')\n",
        "\n",
        "out.close()"
      ],
      "metadata": {
        "id": "5bRexGf6aYXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tanvir_nmt_with_attention.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}